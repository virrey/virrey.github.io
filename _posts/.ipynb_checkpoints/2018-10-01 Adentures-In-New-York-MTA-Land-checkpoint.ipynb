{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import pylab\n",
    "\n",
    "pd.set_option('display.max_columns',40)\n",
    "\n",
    "# jupyter magic to allow displaying figures in notebook and exporting as svg files\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(week_nums):\n",
    "    \"\"\" \n",
    "    Loops through MTA data by taking in a list of filename suffixes.\n",
    "    This then reads into 'dfs' pandas dataframe and returns concatenation of dfs.\n",
    "    \n",
    "    input: list of integer numbers.  must be in format yymmdd.  dates should be a Saturday; files contain data \n",
    "    for that Saturday and the six days after.\n",
    "    \n",
    "    output: pd df\n",
    "    \"\"\"\n",
    "\n",
    "    url = \"http://web.mta.info/developers/data/nyct/turnstile/turnstile_{}.txt\"\n",
    "    dfs = []\n",
    "    for week_num in week_nums:\n",
    "        #print('gathering {}'.format(week_num))\n",
    "        file_url = url.format(week_num)\n",
    "        dfs.append(pd.read_csv(file_url))\n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning / aggregating data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daily_counts(row, max_counter):\n",
    "    \"\"\" \n",
    "    Checks the results of delta calculation for abnormalities and handles them.\n",
    "    Less than zero are flipped to positive. Greater than predefined max are reset to 0\n",
    "    \"\"\"\n",
    "    counter = row[\"ENTRIES\"] - row[\"PREV_ENTRIES\"]\n",
    "    if counter < 0:\n",
    "        # Maybe counter is reversed?\n",
    "        counter = -counter\n",
    "    if counter > max_counter:\n",
    "        #print(row[\"ENTRIES\"], row[\"PREV_ENTRIES\"])\n",
    "        counter = min(row[\"ENTRIES\"], row[\"PREV_ENTRIES\"])\n",
    "        # if current entries is bad, use yesterday's count as proxy\n",
    "    if counter > max_counter:\n",
    "        # Check it again to make sure we are not giving a counter that's too big\n",
    "        return 0\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list defines range of data we want to pull from MTA data extraction using our get_data\n",
    "# function list can be adjusted for different data ranges -- please follow nomenclature\n",
    "# found on MTA website\n",
    "\n",
    "week_nums = [180407, 180414, 180421, 180428, 180505, 180512,\\\n",
    "             180519, 180526, 180602, 180609, 180616, 180623]\n",
    "data = get_data(week_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stripping leading and trailing white space from both the column names and from the text data in column 'STATION'\n",
    "\n",
    "df = data.copy()\n",
    "df.columns = [column.strip() for column in df.columns]\n",
    "df['STATION'].apply(lambda x: x.strip());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one Datetime column from two separate DATE and TIME columns\n",
    "df[\"DATE_TIME\"] = pd.to_datetime(df.DATE + \" \" + df.TIME, \\\n",
    "                format=\"%m/%d/%Y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using C/A - UNIT - SCP - STATION - DATA_TIME as dataframe subset 'key', drop duplicates\n",
    "# Also dropping columns we will not be using (EXITS and DESC)\n",
    "\n",
    "df.sort_values([\"C/A\", \"UNIT\", \"SCP\", \"STATION\", \"DATE_TIME\"],\\\n",
    "               inplace=True, ascending=False)\n",
    "df.drop_duplicates(subset=[\"C/A\", \"UNIT\", \"SCP\", \"STATION\", \"DATE_TIME\"],\\\n",
    "                   inplace=True)\n",
    "df = df.drop([\"EXITS\", \"DESC\"], axis=1, errors=\"ignore\");\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort our \"ENTRIES\". By including \"DATE_TIME\" we setup for filtering .first in the next step.\n",
    "\n",
    "(df\n",
    " .groupby([\"C/A\", \"UNIT\", \"SCP\", \"STATION\", \"DATE_TIME\"])\n",
    " .ENTRIES.count()\n",
    " .reset_index()\n",
    " .sort_values(\"ENTRIES\", ascending=False));\n",
    "# mask =  ((df[\"UNIT\"] == \"R469\") & \n",
    "#          (df[\"SCP\"]==\"00-03-01\") &\n",
    "#         (df[\"STATION\"] == \"RIT-ROOSEVELT\") &\n",
    "#         (df[\"DATE\"] == \"06/22/2018\"))\n",
    "# df[mask].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort ENTRIES and only keep the first value (ending value for the day)\n",
    "df_daily = (df\n",
    "            .groupby([\"C/A\", \"UNIT\", \"SCP\", \"STATION\", \"DATE\"])\n",
    "            .ENTRIES\n",
    "            .first()\n",
    "            .reset_index()\n",
    "            .copy()\n",
    "            )\n",
    "\n",
    "# mask =  ((df_daily[\"UNIT\"] == \"R469\") & \n",
    "#         (df_daily[\"SCP\"]==\"00-03-01\") &\n",
    "#         (df_daily[\"STATION\"] == \"RIT-ROOSEVELT\") &\n",
    "#         (df_daily[\"DATE\"] == \"06/22/2018\"))\n",
    "# df_daily[mask].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the previous row's DATE AND ENTRIES and pull it into current column for all columns.\n",
    "df_daily[[\"PREV_DATE\", \"PREV_ENTRIES\"]] = (df_daily.groupby([\"C/A\", \"UNIT\", \"SCP\", \"STATION\"])[\"DATE\", \"ENTRIES\"]\\\n",
    "                                           .transform(lambda grp: grp.shift(1)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our get_daily_counts function to clean up delta between previous and current ENTRIES.\n",
    "df_daily[\"DAILY_ENTRIES\"] = df_daily.apply(get_daily_counts, axis=1, max_counter=1000000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dataframe concerning only STATION and DATE attributes while\n",
    "# summing on our DAILY ENTRIES. This will sum all of the turnstiles from\n",
    "# each station.\n",
    "df_daily_sta = (df_daily\n",
    "                .groupby(['STATION', 'DATE'])['DAILY_ENTRIES']\n",
    "                .sum()\n",
    "                .reset_index()\n",
    "                .sort_values(['DATE'])\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating descriptive statistics / charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframes for top stations by median (top 20 and top 5)\n",
    "df_medians = (df_daily_sta\n",
    "              .groupby(['STATION'])['DAILY_ENTRIES']\n",
    "              .agg(['median'])\n",
    "              .reset_index()\n",
    "              .sort_values(['median'], ascending=False)\n",
    "              )\n",
    "df_medians_top20 = df_medians[:20]\n",
    "df_medians_top5 = df_medians[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the top 20 stations dataframe to make a bar plot with appropriate labeling\n",
    "df_medians_top20.plot.bar(x='STATION',y='median',color='blue')\n",
    "plt.xlabel('Station')\n",
    "plt.ylabel('Median')\n",
    "plt.title(\"Median Ridership for Top 20 Stations\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('medians_total.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a \"Weekly Medians\" dataframe from our prior 'daily_statons' dataframe.\n",
    "# Create a \"WEEK\" column via conversion of \"DATE\" to a datetime object and extracting the week\n",
    "# Create a \"Top 5 Stations Weekly Medians\" df based on the top 5 Stations previously identified\n",
    "df_wk_medians = (df_daily_sta\n",
    "                 .groupby(['STATION','DATE'])['DAILY_ENTRIES']\\\n",
    "                 .sum()\n",
    "                 .reset_index()\n",
    "                 .sort_values(['DAILY_ENTRIES'], ascending=False) \n",
    "                 )\n",
    "df_wk_medians['WEEK'] = (pd.to_datetime(df_wk_medians['DATE'], format=\"%m/%d/%Y\")\\\n",
    "                           .dt\n",
    "                           .week\n",
    "                           )\n",
    "\n",
    "df_wk_medians = (df_wk_medians\n",
    "                 .groupby(['STATION','WEEK'])['DAILY_ENTRIES']\\\n",
    "                 .agg(['median'])\n",
    "                 .reset_index()\n",
    "                 .sort_values(['median'],ascending=False)\n",
    "                 )\n",
    "\n",
    "df_wk_medians_top5 = (df_wk_medians.loc\n",
    "                      [(df_wk_medians.STATION.isin(df_medians_top5.STATION)) &\n",
    "                       (df_wk_medians.WEEK.isin(range(14,25))), :]\n",
    "                      .reset_index()\n",
    "                      .sort_values(['WEEK','STATION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subplot loop to plot the top 5 lines. Marker list is rotated with a counter.\n",
    "# This made the lines easier to differntiate vs just colors.\n",
    "mrk = [\".\",\",\",\"o\",\"v\",\"^\",\"<\",\">\",\"1\",\"2\",\"3\",\"4\",\"8\",\"s\",\"p\",\"P\",\"*\",\"h\",\"H\",\"+\",\"x\",\"X\",\"D\",\"d\",\"|\"]\n",
    "mrkn = 0\n",
    "a = df_wk_medians_top5\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "\n",
    "for key, grp in a.groupby(['STATION']):\n",
    "    mrkn = mrkn % 24\n",
    "    ax = grp.plot(ax=ax, kind='line', x='WEEK', y='median', label=key, marker=mrk[mrkn], legend=None)\n",
    "    mrkn +=1\n",
    "    \n",
    "plt.xticks(a.WEEK)\n",
    "plt.legend(loc='upper left')\n",
    "plt.savefig('medians_byWeek.svg')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same motion as creating the \"Weekly Medians\" df except we are\n",
    "# getting the IQRs instead of medians.\n",
    "df_wk_iqr = (df_daily_sta\n",
    "             .groupby(['STATION','DATE'])['DAILY_ENTRIES']\\\n",
    "             .sum()\n",
    "             .reset_index()\n",
    "             .sort_values(['DAILY_ENTRIES'], ascending=False)\n",
    "             )\n",
    "\n",
    "df_wk_iqr['WEEK'] = (pd.to_datetime(df_wk_iqr['DATE'], format=\"%m/%d/%Y\")\\\n",
    "                       .dt\n",
    "                       .week\n",
    "                       )\n",
    "\n",
    "df_wk_iqr = (df_wk_iqr\n",
    "             .groupby(['STATION','WEEK'])['DAILY_ENTRIES']\\\n",
    "             .agg([stats.iqr])\n",
    "             .reset_index()\n",
    "             .sort_values(['iqr'],ascending=False)\n",
    "             )\n",
    "\n",
    "df_wk_iqr_top5 = (df_wk_iqr\n",
    "                  .loc[(df_wk_iqr.STATION.isin(df_medians_top5.STATION)) &\n",
    "                       (df_wk_iqr.WEEK.isin(range(14,25))), :]\n",
    "                  .reset_index()\n",
    "                  .sort_values(['WEEK','STATION'])\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is the same motion as the previous plot loop except we're plotting IQRs\n",
    "# instead of medians for the top 5 stations.\n",
    "b = df_wk_iqr_top5\n",
    "mrkn = 0\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "\n",
    "for key, grp in b.groupby(['STATION']):\n",
    "    mrkn = mrkn % 24\n",
    "    ax = grp.plot(ax=ax, kind='line', x='WEEK', y='iqr', label=key, marker=mrk[mrkn])\n",
    "    mrkn += 1\n",
    "\n",
    "plt.xticks(b.WEEK)\n",
    "plt.legend(loc='upper left')\n",
    "plt.savefig('IQR_byWeek.svg')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading / Cleaning / Attaching / Using Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in weather data\n",
    "data_w = pd.read_csv('NOAA_weather.csv', low_memory = False)\n",
    "\n",
    "#values don't need to be transformed but columns need to be made clearer\n",
    "data_w = (\n",
    "    data_w.rename(columns = {'AWND': 'avg_wind_spd', 'PGTM': 'peak_wind_gust_time', 'PRCP': 'precipitation', \n",
    "                            'SNOW':'snowfall', 'SNWD':'snow_depth', 'TAVG':'temp_avg', 'TMAX':'temp_max',\n",
    "                            'TMIN':'temp_min', 'TOBS':'temp_moment', 'WESD':'water_on_ground', \n",
    "                            'WSF2':'wind_fst_wmin', 'WSF5':'wind_fst_5sec'})\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset includes weather info from Princeton NJ to New Haven, CT, so we'll want to filter by lat/long \n",
    "data_w['LAT_LONG'] = [coords for coords in zip(data_w.LATITUDE, data_w.LONGITUDE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WHY TYPE ALL CAPS WHEN YOU DON'T HAVE TO\n",
    "data_w.columns = [column.lower() for column in data_w.columns]\n",
    "\n",
    "data_w.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "filtering the weather data to roughly the NYC metro area which the MTA serves\n",
    "it can be approximate boundaries b/c \n",
    "    - we will aggregate the values together\n",
    "    - since we're aggregating a large area in the hundreds of square miles,\n",
    "    being off by a few miles here and there on the edges is a small % error\n",
    "    - this is weather data, and the values don't change appreciably in small areas\n",
    "'''\n",
    "\n",
    "mask = ((data_w.latitude < 40.87305) &\n",
    "        (data_w.latitude > 40.57164) &\n",
    "        (data_w.longitude < -73.21914) &\n",
    "        (data_w.longitude > -74.0406)\n",
    "       )\n",
    "       \n",
    "df_w = data_w.loc[mask, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter to the year of the MTA data just to make things cleaner\n",
    "df_w = df_w[df_w.year == 2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "these variables were the most obvious to figure out from the weather data dictionary\n",
    "and cover the basic weather variables that you would think people would use \n",
    "to gauge whether they take the subway in a given day: \n",
    "    wind, rain, temp\n",
    "'''\n",
    "df_w_agg = (df_w.groupby('date')\n",
    "            ['avg_wind_spd', 'precipitation', 'temp_avg', 'temp_max', 'temp_min', 'wind_fst_wmin', 'wind_fst_5sec']\n",
    "            .median()\n",
    "            .reset_index()\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forget if there was a keyboard slip somewhere or this was in the original data\n",
    "df_w_agg = df_w_agg.rename(columns = {'wind_fst_wmin': 'wind_fst_2min'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the date values acting as the key we're merging on in the MTA data is in a different order\n",
    "df_w_agg['merge_date'] = [date[5:7]+'/'+date[8:]+'/'+date[0:4] for date in df_w_agg.date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#don't need the column of dates in the weather dataset format anymore\n",
    "df_w_agg = df_w_agg.drop('date', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge check: setting the stage\n",
    "print(df_daily.describe())\n",
    "print\n",
    "print(len(df_daily))\n",
    "df_daily.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging MTA daily ridership data with daily weather data with date as key\n",
    "df_daily_w = df_daily.merge(df_w_agg, left_on = 'DATE', right_on = 'merge_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping superfluous key column\n",
    "df_daily_w =df_daily_w.drop('merge_date', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge check: checks out\n",
    "print(df_daily_w.describe())\n",
    "print\n",
    "print(len(df_daily_w))\n",
    "df_daily_w.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data into the top 5 stations identified above to see how weather affects them\n",
    "#list comprehensions what are list comprehensions\n",
    "\n",
    "df_daily_w_1 = df_daily_w.loc[df_daily_w.STATION.isin(['34 ST-PENN STA']), :]\n",
    "df_daily_w_2 = df_daily_w.loc[df_daily_w.STATION.isin(['GRD CNTRL-42 ST']), :]\n",
    "df_daily_w_3 = df_daily_w.loc[df_daily_w.STATION.isin(['34 ST-HERALD SQ']), :]\n",
    "df_daily_w_4 = df_daily_w.loc[df_daily_w.STATION.isin(['23 ST']), :]\n",
    "df_daily_w_5 = df_daily_w.loc[df_daily_w.STATION.isin(['14 ST-UNION SQ']), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "creating a df for a correlation matrix from each dataset above\n",
    "dropping columns except for the columns we're interested in correlating \n",
    "(weather variables and daily ridership)\n",
    "'''\n",
    "\n",
    "corrs1 = (df_daily_w_1\n",
    "          .corr()\n",
    "          .drop(['PREV_ENTRIES', 'ENTRIES'], axis = 1)\n",
    "          )\n",
    "corrs1 = corrs1.drop(['ENTRIES', 'PREV_ENTRIES'], axis = 0)\n",
    "\n",
    "\n",
    "\n",
    "corrs2 = (df_daily_w_2\n",
    "          .corr()\n",
    "          .drop(['PREV_ENTRIES', 'ENTRIES'], axis = 1)\n",
    "          )\n",
    "corrs2 = corrs2.drop(['ENTRIES', 'PREV_ENTRIES'], axis = 0)\n",
    "\n",
    "\n",
    "\n",
    "corrs3 = (df_daily_w_3\n",
    "          .corr()\n",
    "          .drop(['PREV_ENTRIES', 'ENTRIES'], axis = 1)\n",
    "          )\n",
    "corrs3 = corrs3.drop(['ENTRIES', 'PREV_ENTRIES'], axis = 0)\n",
    "\n",
    "\n",
    "\n",
    "corrs4 = (df_daily_w_4\n",
    "          .corr()\n",
    "          .drop(['PREV_ENTRIES', 'ENTRIES'], axis = 1)\n",
    "          )\n",
    "corrs4 = corrs4.drop(['ENTRIES', 'PREV_ENTRIES'], axis = 0)\n",
    "\n",
    "\n",
    "\n",
    "corrs5 = (df_daily_w_5\n",
    "          .corr()\n",
    "          .drop(['PREV_ENTRIES', 'ENTRIES'], axis = 1)\n",
    "          )\n",
    "corrs5 = corrs5.drop(['ENTRIES', 'PREV_ENTRIES'], axis = 0)\n",
    "\n",
    "#these will be axis labels\n",
    "names = corrs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "following cells make the correlation matrix graphs\n",
    "they're broken up so that each graphic could be inspected indivudally\n",
    "and if one f'ed up wouldn't have to re-run all the others\n",
    "'''\n",
    "\n",
    "#plot first matrix\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(corrs1, vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "ticks = np.arange(0,len(names),1)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(names, rotation = 90)\n",
    "ax.set_yticklabels(names)\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation-1.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot second matrix\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(corrs2, vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "ticks = np.arange(0,len(names),1)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(names, rotation = 90)\n",
    "ax.set_yticklabels(names)\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation-2.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot three matrix\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(corrs3, vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "ticks = np.arange(0,len(names),1)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(names, rotation = 90)\n",
    "ax.set_yticklabels(names)\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation-3.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot fourth matrix\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(corrs4, vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "ticks = np.arange(0,len(names),1)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(names, rotation = 90)\n",
    "ax.set_yticklabels(names)\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation-4.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot fifth matrix\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(corrs2, vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "ticks = np.arange(0,len(names),1)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(names, rotation = 90)\n",
    "ax.set_yticklabels(names)\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation-5.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
